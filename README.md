# TextGenerationUsingGPT-2
Generating similar text after training GPT-2 model on a given text

The project uses the GPT-2 model. The model is fine-tuned (transfer learning) on input text. In the case of this project, the input text is the entirety of The Wonderful Wizard of Oz. The introduction and index of the book was removed to make the text consistent.

The model was fine-tuned for 1400 epochs.

# To run the notebook
Open the notebook in google colab. Put the input text for training in the base folder of Google Drive (Drive). In the notebook, change the `file_name` to the name of your input text. Then run the notebook. The notebook will ask for access to your drive. Allow it.
